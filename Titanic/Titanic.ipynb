{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "titanic = pd.read_csv('Data/train.csv', index_col='PassengerId')\n",
    "y = titanic['Survived']\n",
    "X = titanic.drop('Survived', axis=1).copy()\n",
    "\n",
    "# The individual cabins probably won't tell us much, but the letter block might\n",
    "# We'll use categorical dummies here so the missing values shouldn't be terribly important\n",
    "cabin_ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "X['Cabin block'] = X['Cabin'].str[0].fillna('unknown')\n",
    "cabin_dummies = pd.DataFrame(cabin_ohe.fit_transform(X[['Cabin block']]),\n",
    "                             columns=['block_' + c for c in cabin_ohe.categories_[0]],\n",
    "                             index=X.index)\n",
    "X = pd.concat([X, cabin_dummies], axis=1)\n",
    "X = X.drop(['Cabin', 'Cabin block'], axis=1)\n",
    "\n",
    "\n",
    "# There are only two missing embarcation locations, so it should be fine with dummies\n",
    "embark_ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "embark_dummies = pd.DataFrame(embark_ohe.fit_transform(X[['Embarked']].fillna('S')),\n",
    "                              columns=['embark_' + c for c in embark_ohe.categories_[0]],\n",
    "                              index=X.index)\n",
    "X = pd.concat([X, embark_dummies], axis=1)\n",
    "X = X.drop('Embarked', axis=1)\n",
    "\n",
    "\n",
    "# The tickets always have number components except for 4 \"LINE\" tickets - let's try filling those with 0\n",
    "X['Ticket #'] = X['Ticket'].str.split(' ').apply(lambda x: [t for t in x if t.isdigit()]).str[0].fillna(0)\n",
    "X['Ticket #'] = pd.to_numeric(X['Ticket #'])\n",
    "X = X.drop('Ticket', axis=1)\n",
    "\n",
    "# Some feature engineering to get the number of like-surnames in the ship - who knows\n",
    "X['Surname'] = X['Name'].str.split(',').str[0]\n",
    "surname_counts = X['Surname'].value_counts()\n",
    "X['Surname count'] = X['Surname'].apply(lambda x: surname_counts.loc[x])\n",
    "X = X.drop(['Name', 'Surname'], axis=1)\n",
    "\n",
    "# Sex should be a simple matter of categorical encoding\n",
    "sex_le = LabelEncoder()\n",
    "X['Sex'] = sex_le.fit_transform(X['Sex'])\n",
    "\n",
    "# Age is tricky - there are a lot of missing values and this could prove critical.\n",
    "# Let's try regressing against it from the other variables\n",
    "# First, let's round it since the exact value probably doesn't matter much and the .5s are rounded or infants\n",
    "# But let's keep the information for approximate age since people whose exact age we don't know probably died.\n",
    "# (Side note: including this information is probably leaking target data)\n",
    "X['Approx age'] = X['Age'] % 1 == 0.5\n",
    "X['Age'] = X['Age'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_age = X[X['Age'].notnull()]\n",
    "age_X = known_age.drop('Age', axis=1)\n",
    "age_y = known_age['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rf_params = [{'max_features': [0.2, 0.4, 0.5, 0.6, 0.8],\n",
    "             'n_estimators': [50, 100, 200, 300]}]\n",
    "rf_grid = GridSearchCV(rfr, rf_params, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "\n",
    "ela = ElasticNet()\n",
    "ela_params = [{'alpha': [0.001, 0.01, 0.1, 1, 100, 1000]},\n",
    "             {'l1_ratio': [0, 0.2, 0.4, 0.6, 0.8, 1.]}]\n",
    "ela_grid = GridSearchCV(ela, ela_params, cv=5, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   23.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-165.48807639669027\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_grid.fit(age_X, age_y)\n",
    "print(rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-166.13317360179266\n",
      "Wall time: 440 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  53 out of  60 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ela_grid.fit(age_X, age_y)\n",
    "print(ela_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RF performed better, so let's use that to make our predictions on age\n",
    "age_estimator = rf_grid.best_estimator_\n",
    "age_predictions = age_estimator.predict(X[X['Age'].isnull()].drop('Age', axis=1))\n",
    "X.loc[X['Age'].isnull(), 'Age'] = age_predictions.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   19.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8451321323206328\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now to train the actual model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc_params = [{'max_features': [0.2, 0.4, 0.5, 0.6, 0.8],\n",
    "             'n_estimators': [50, 100, 200, 300]}]\n",
    "rfc_grid = GridSearchCV(rfc, rfc_params, verbose=1, cv=5, n_jobs=-1)\n",
    "\n",
    "rfc_grid.fit(X, y)\n",
    "print(rfc_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8024606113866047\n",
      "Wall time: 780 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 123 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now to train the actual model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr_params = [{'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "               'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "lr_grid = GridSearchCV(lr, lr_params, verbose=1, cv=5, n_jobs=-1)\n",
    "\n",
    "# Standardizing our numeric features should improve our results in a linear model like Logistic Regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "X_std = std.fit_transform(X)\n",
    "\n",
    "lr_grid.fit(X_std, y)\n",
    "print(lr_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8473793233318687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:   12.1s finished\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgbc = xgb.XGBClassifier()\n",
    "xgbc_params = [{'n_estimators': [50, 100, 200, 300],\n",
    "                'gamma': [0, 0.2, 0.4, 0.6, 0.8, 0.9, 1]}]\n",
    "xgb_grid = GridSearchCV(xgbc, xgbc_params, cv=5, verbose=1, n_jobs=-1)\n",
    "xgb_grid.fit(X, y)\n",
    "\n",
    "print(xgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Data/test.csv', index_col='PassengerId')\n",
    "X = test.copy()\n",
    "\n",
    "X['Cabin block'] = X['Cabin'].str[0].fillna('unknown')\n",
    "cabin_dummies = pd.DataFrame(cabin_ohe.transform(X[['Cabin block']]),\n",
    "                             columns=['block_' + c for c in cabin_ohe.categories_[0]],\n",
    "                             index=X.index)\n",
    "X = pd.concat([X, cabin_dummies], axis=1)\n",
    "X = X.drop(['Cabin', 'Cabin block'], axis=1)\n",
    "\n",
    "\n",
    "# There are only two missing embarcation locations, so it should be fine with dummies\n",
    "embark_dummies = pd.DataFrame(embark_ohe.transform(X[['Embarked']].fillna('S')),\n",
    "                              columns=['embark_' + c for c in embark_ohe.categories_[0]],\n",
    "                              index=X.index)\n",
    "X = pd.concat([X, embark_dummies], axis=1)\n",
    "X = X.drop('Embarked', axis=1)\n",
    "\n",
    "\n",
    "# The tickets always have number components except for 4 \"LINE\" tickets - let's try filling those with 0\n",
    "X['Ticket #'] = X['Ticket'].str.split(' ').apply(lambda x: [t for t in x if t.isdigit()]).str[0].fillna(0)\n",
    "X['Ticket #'] = pd.to_numeric(X['Ticket #'])\n",
    "X = X.drop('Ticket', axis=1)\n",
    "\n",
    "# Some feature engineering to get the number of like-surnames in the ship - who knows\n",
    "X['Surname'] = X['Name'].str.split(',').str[0]\n",
    "X['Surname count'] = X['Surname'].apply(lambda x: surname_counts.get(x)).fillna(1)\n",
    "X = X.drop(['Name', 'Surname'], axis=1)\n",
    "\n",
    "# Sex should be a simple matter of categorical encoding\n",
    "X['Sex'] = sex_le.transform(X['Sex'])\n",
    "\n",
    "# Age is tricky - there are a lot of missing values and this could prove critical.\n",
    "# Let's try regressing against it from the other variables\n",
    "# First, let's round it since the exact value probably doesn't matter much and the .5s are rounded or infants\n",
    "# But let's keep the information for approximate age since people whose exact age we don't know probably died.\n",
    "# (Side note: including this information is probably leaking target data)\n",
    "X['Approx age'] = X['Age'] % 1 == 0.5\n",
    "X['Age'] = X['Age'].round()\n",
    "age_predictions = age_estimator.predict(X[X['Age'].isnull()].drop('Age', axis=1))\n",
    "X.loc[X['Age'].isnull(), 'Age'] = age_predictions.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Survived'] = xgb_grid.predict(X)\n",
    "test['Survived'].to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
